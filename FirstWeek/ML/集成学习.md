## 集成学习

集成学习是一种技术框架，它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务，一般结构是：先产生一组“个体学习器”，再用某种策略将它们结合起来，目前，有三种常见的集成学习框架（策略）：bagging，boosting和stacking也就是说，集成学习有两个主要的问题需要解决，第一是如何得到若干个个体学习器，第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器
 集成学习是指将若干弱分类器组合之后产生一个强分类器。弱分类器（weak learner）指那些分类准确率只稍好于随机猜测的分类器（error rate < 50%）。如今集成学习有两个流派，一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合，随机森林算法就属于bagging派系；另一个是boosting派系，它的特点是各个弱学习器之间有依赖关系，Adaboost算法就属于boosting派系。在实现集成学习算法时，很重要的一个核心就是如何实现数据的多样性，从而实现弱分类器的多样性
集成学习有如下的特点：
- 将多个分类方法聚集在一起，以提高分类的准确率（这些算法可以是不同的算法，也可以是相同的算法。）；
- 集成学习法由训练数据构建一组基分类器，然后通过对每个基分类器的预测进行投票来进行分类；
- 严格来说，集成学习并不算是一种分类器，而是一种分类器结合的方法；
- 通常一个集成分类器的分类性能会好于单个分类器； 
    - 如果把单个分类器比作一个决策者的话，集成学习的方法就相当于多个决策者共同进行一项决策。"
 ### Bagging是基于自助采样法采样出`T`个含`m`个训练样本的采样集，然后基于每个采样集训练一个基学习器，再将这些基学习器进行结合。
Bagging（装袋算法）的集成学习方法非常简单，假设我们有一个数据集D，使用Bootstrap sample（有放回的随机采样，这里说明一下，有放回抽样是抽一个就放回一个，然后再抽，而不是这个人抽10个，再放回，下一个继续抽，它是每一个样本被抽中概率符合均匀分布）的方法取了k个数据子集（子集样本数都相等）：D1，D2，…，Dk，作为新的训练集，我们使用这k个子集分别训练一个分类器（使用分类、回归等算法），最后会得到k个分类模型。我们将测试数据输入到这k个分类器，会得到k个分类结果，比如分类结果是0和1，那么这k个结果中谁占比最多，那么预测结果就是谁。
#### 自助采样法
自助采样法：给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m次随机采样操作，我们得到含m个样本的采样集，初始训练集中有的样本多次出现，有的则从未出现。但是还需要计算出样本在采样中始终不被采到的概率为
    $$
     {(1-\frac{1} m)}^m = {\\frac1 e  }
    $$
给定M个数据集，有放回的随机抽取M个数据，假设如此抽取3组，3组数据一定是有重复的，所以先去重。去重后得到3组数据，每组数据量分别是s1,s2,s3，然后三组分别训练组合成一个强模型。大致过程如下：
- 对于给定的训练样本S,每轮从训练样本S中采用有放回抽样(Booststraping)的方式抽取M个训练样本,共进行n轮，得到了n个样本集合，需要注意的是这里的n个训练集之间是相互独立的。
- 在获取了样本集合之后，每次使用一个样本集合得到一个预测模型，对于n个样本集合来说，我们总共可以得到n个预测模型。
- 如果我们需要解决的是分类问题，那么我们可以对前面得到的n个模型采用投票的方式得到分类的结果，对于回归问题来说，我们可以采用计算模型均值的方法来作为最终预测的结果
    ![bagging](bagging)
优缺点：
- Bagging通过降低基分类器的方差，改善了泛化误差； 
- 其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起； 
- 由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例

1.优点，可以减小方差和减小过拟合

2.缺点，自助采样法改变了数据原有的分布，因此在一定程度上引入了偏差，对最终的结果预测会造成一定程度的影响

- bias（偏差）描述的是根据样本拟合出的模型的输出预测结果的期望与样本真实结果的差距，简单讲，就是在样本上拟合的好不好。要想在bias上表现好，low bias，就得复杂化模型，增加模型的参数，但这样容易过拟合 (overfitting)，过拟合对应上图是high variance，点很分散。low bias对应就是点都打在靶心附近，所以瞄的是准的，但手不一定稳。
- varience（方差）描述的是样本上训练出来的模型在测试集上的表现，要想在variance上表现好，low varience，就要简化模型，减少模型的参数，但这样容易欠拟合(unfitting)，欠拟合对应上图是high bias，点偏离中心。low variance对应就是点都打的很集中，但不一定是靶心附近，手很稳，但是瞄的不准。

![avatar](modelTeat)

#### bagging与boosting

bagging是对许多强（甚至过强）的分类器求平均。在这里，每个单独的分类器的bias都是低的，平均之后bias依然低；而每个单独的分类器都强到可能产生overfitting的程度，也就是variance高，求平均的操作起到的作用就是降低这个variance。

boosting是把许多弱的分类器组合成一个强的分类器。弱的分类器bias高，而强的分类器bias低，所以说boosting起到了降低bias的作用。variance不是boosting的主要考虑因素。Boosting 则是迭代算法，每一次迭代都根据上一次迭代的预测结果对样本进行加权，所以随着迭代不断进行，误差会越来越小，所以模型的 bias 会不断降低。这种算法无法并行，例子比如Adaptive Boosting

Bagging和Rand Forest(随机森林)
1）Rand Forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而Bagging一般选取比输入样本的数目少的样本
2）bagging是用全部特征来得到分类器，而Rand Forest是需要从全部特征中选取其中的一部分来训练得到分类器； 一般Rand forest效果比Bagging效果好！

 