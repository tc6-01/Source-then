{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K近邻算法（欧氏距离）\n",
    "存在一个样本数据集合，也称作为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一个数据与所属分类的对应关系。输入没有标签的新数据后，将新的数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本最相似数据（(最近邻)计算两点之间的距离）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建函数集\n",
    "import numpy as np\n",
    "def createData():\n",
    "    data=np.array([[1.,101],[5.,89],[108.,5],[115.,8],[100,40]])\n",
    "    labels=['LoverView','LoverView','ActionView','ActionView','ActionView']\n",
    "    return data,labels \n",
    "    # 生成数据集以及对应的数据标签\n",
    "np.tile()\n",
    "group ,labels = createData()\n",
    "print(group)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K近邻算法步骤\n",
    "- 计算已知类别数据集中的点与当前点之间的距离；\n",
    "- 按照距离递增次序排序；\n",
    "- 选取与当前点距离最小的k个点；\n",
    "- 确定前k个点所在类别的出现频率；\n",
    "- 返回前k个点所出现频率最高的类别作为当前点的预测分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "def KNN(test,Train,labels,K):\n",
    "    # 获取样本个数，即属性行数\n",
    "    row = Train.shape[0]\n",
    "    # 测试集将规模扩展至训练集一致\n",
    "    ready = np.tile(test,(row,1)) - Train\n",
    "    # np.tile(array,(lay,row))将array的列复制lay次，行复制row次\n",
    "    sqDis = ready**2\n",
    "    befDis = sqDis.sum(axis=1)\n",
    "    #  axis=1行内元素相加，axis=0列内元素相加\n",
    "    Dis = befDis**0.5\n",
    "    # 开方得到欧氏距离\n",
    "    sortIndex = Dis.argsort()\n",
    "    # 返回排序后的Index\n",
    "    # 按照距离进行排序并返回对应的索引\n",
    "    classCount={}\n",
    "    # 设置字典记录出现频率\n",
    "    for i in range(K):\n",
    "        # 取出前K个元素的标记\n",
    "        label = labels[sortIndex[i]]\n",
    "        classCount[label] = classCount.get(label,0)+1\n",
    "        # 0是默认值，没有找到返回默认值\n",
    "    print(classCount)\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    print(sortedClassCount)\n",
    "    # operator.itemgetter(1)是字典的value，operator.itemgetter(0)是字典的key\n",
    "    return sortedClassCount[0][0]\n",
    "   \n",
    "test = [40,49]\n",
    "testClass = KNN(test,group,labels,3)\n",
    "print(\"this is a\",testClass)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整的K-近邻算法\n",
    "- 收集数据：可以使用爬虫进行数据的收集，也可以使用第三方提供的免费或收费的数据。一般来讲，数据放在txt文本文件中，按照一定的格式进行存储，便于解析及处理。\n",
    "- 准备数据：使用Python解析、预处理数据。\n",
    "- 分析数据：可以使用很多方法对数据进行分析，例如使用Matplotlib将数据可视化。\n",
    "- 测试算法：计算错误率。\n",
    "- 使用算法：错误率在可接受范围内，就可以运行k-近邻算法进行分类。\n",
    "\n",
    "### 海伦约会实例---预测下一个特征的人会是她的理想吗？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def file2matrix(filename):\n",
    "\t#打开文件,此次应指定编码，\n",
    "    \n",
    "    fr = open(filename,'r',encoding = 'utf-8')\n",
    "\t#读取文件所有内容\n",
    "    arrayOLines = fr.readlines()\n",
    "    #针对有BOM的UTF-8文本，应该去掉BOM，否则后面会引发错误。\n",
    "    arrayOLines[0]=arrayOLines[0].lstrip('\\ufeff')\n",
    "\t#得到文件行数\n",
    "    numberOfLines = len(arrayOLines)\n",
    "\t#返回的NumPy矩阵,解析完成的数据:numberOfLines行,3列\n",
    "    returnMat = np.zeros((numberOfLines,3))\n",
    "\t#返回的分类标签向量\n",
    "    classLabelVector = []\n",
    "\t#行的索引值\n",
    "    index = 0\n",
    "\n",
    "    for line in arrayOLines:\n",
    "\t\t#s.strip(rm)，当rm空时,默认删除空白符(包括'\\n','\\r','\\t',' ')\n",
    "        line = line.strip()\n",
    "\t\t#使用s.split(str=\"\",num=string,cout(str))将字符串根据'\\t'分隔符进行切片。\n",
    "        listFromLine = line.split('\\t')\n",
    "\t\t#将数据前三列提取出来,存放到returnMat的NumPy矩阵中,也就是特征矩阵\n",
    "        returnMat[index,:] = listFromLine[0:3]\n",
    "\t\t#根据文本中标记的喜欢的程度进行分类,1代表不喜欢,2代表魅力一般,3代表极具魅力   \n",
    "\t\t# 对于datingTestSet2.txt  最后的标签是已经经过处理的 标签已经改为了1, 2, 3\n",
    "        if listFromLine[-1] == 'didntLike':\n",
    "            classLabelVector.append(1)\n",
    "        elif listFromLine[-1] == 'smallDoses':\n",
    "            classLabelVector.append(2)\n",
    "        elif listFromLine[-1] == 'largeDoses':\n",
    "            classLabelVector.append(3)\n",
    "        index += 1\n",
    "    return returnMat, classLabelVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = open(\"L:/Machine-Learning/kNN/2.海伦约会/datingTestSet.txt\",'r')\n",
    "file = fr.readlines()\n",
    "length = len(file)\n",
    "# print(file,'--------')\n",
    "margtix=np.zeros((length,3))\n",
    "index =0\n",
    "classr = []\n",
    "for line in file:\n",
    "    # print(line)\n",
    "    line=line.strip()\n",
    "    line=line.split('\\t')\n",
    "    margtix[index,:]=line[0:3]\n",
    "    if line[-1] == 'didntLike':\n",
    "        classr.append(1)\n",
    "    elif line[-1] == 'smallDoses':\n",
    "        classr.append(2)\n",
    "    elif line[-1] == 'largeDoses':\n",
    "        classr.append(3)\n",
    "    index += 1\n",
    "print(classr)\n",
    "\n",
    "\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# First create some toy data:\n",
    "x = np.linspace(0, 2*np.pi, 400)\n",
    "y = np.sin(x**2)\n",
    "\n",
    "# Create just a figure and only one subplot\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2,sharex=False, sharey=False, figsize=(13,8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化处理\n",
    "使得预处理的数据被限制在一定的范围内，从而消除奇异样本数据导致的不良影响\n",
    "- 提升模型的收敛速度\n",
    "    非常熟悉的梯度等高线图，当我们使用梯度下降法寻找系数最优解经常会遇到。你可以想象我们模型在在寻找系数最优解的过程中就相当于在这个等高线图上蹦跶（蹦跶的强度就是我们的学习率），他为什么蹦跶？因为他要找到最凹的地方，也就是等高线中中间最小的圈圈。标准化前，由于变量的单位相差很大，导致了椭圆型的梯度轮廓。标准化后，把变量变成统一单位，产生了圆形轮廓。由于梯度下降是按切线方向下降，所以导致了系统在椭圆轮廓不停迂回地寻找最优解，而圆形轮廓就能轻松找到了。还有一种比较极端的情况，有时没做标准化，模型始终找不到最优解，一直不收敛。\n",
    "- 提升模型的准确度\n",
    "    当我们模型会考虑到特征取值的时候，归一化、标准化就显得十分必要。因为有时候，不同特征之间，他们的取值范围会相差很大。如果这时候不做点什么的话，模型会自然地对取值大的特征有偏向性，忽略了取值小的特征，这是对取值范围小的特征的不公平！为了让他们受到公平对待，归一化可以实现。\n",
    "\n",
    "一些需要归一化的算法\n",
    "\n",
    "|需要|\t不需要|\n",
    "|----|----|\n",
    "|线性回归\t|决策树|\n",
    "|逻辑回归\t|随机森林|\n",
    "|SVM|\t朴素贝叶斯|\n",
    "|KNN|\tXGBoost|\n",
    "|K-MEANS|\tLightGBM|\n",
    "|PCA\t||\n",
    "|Adaboost||\t\n",
    "|神经网络||\t\n",
    "|GBDT\t| |\n",
    "\n",
    "需要的原因\n",
    "\n",
    "| 算法 |需要的原因| \n",
    "| ---- | ---- |\n",
    "|线性回归|     学习率众口难调。在寻找损失函数最小值的时候，对所有特征使用的都是同等学习率，如果学习率对于取值范围比较大的特征来说能够很好地逼近损失函数的极小值的话，对于本身取值范围较小的特征来说，学习率就可能偏大，蹦跶过猛，错过他的最小值。 |   \t\n",
    "|逻辑回归\t|其实不是所有逻辑回归都需要归一化、标准化处理，这取决于是否使用了正则化。有正则化，需要；无正则化，非必须。因为不用正则时，我们的损失函数只是仅仅在度量预测与真实的差距，加上正则后，我们的损失函数除了要度量上面的差距外，还要度量参数值是否足够小。而参数值的大小程度或者说大小的级别是与特征的数值范围相关的。只是进行标准化后，得出的参数值的大小可以反应出不同特征对样本label的贡献度，方便我们进行特征筛选。如果不做标准化，是不能这样来筛选特征的。使用正则化的时候，如果遇到不同特征取值范围差距较大的时候，在L1正则时，我们是简单将参数的绝对值相加，因为它们的大小级别不一样，就会导致L1最后只会对那些级别比较大的参数有作用，那些小的参数都被忽略了。|\n",
    "|SVM|\t道理与之前的原因类似，在求最优超平面时，需要进行求偏导，找极值。|\n",
    "|KNN|\t因为KNN和K-means的原因类似，就放在一起说了。你会发现这两个算法，虽然一个是监督学习算法，另一个是非监督学习算法，但他们都与距离有关。就欧式距离而言，某个特征的取值普遍较大的话，相当于增加了其在距离计算中的权重，如果有明确的先验知识表明某个特征很重要，那么适当增加其权重可能有正向效果，但如果没有这样的先验，或者目的就是想知道哪些特征更重要，那么就需要先归一化、标准化，对各维特征等而视之。|\n",
    "|K-MEANS|   |\n",
    "|PCA|\tPCA通常是用于高维数据的降维，它可以将原来高维的数据投影到某个低维的空间上并使得其方差尽量大。如果数据其中某一特征（矩阵的某一列）的数值特别大，那么它在整个误差计算的比重上就很大，那么可以想象在投影到低维空间之后，为了使低秩分解逼近原数据，整个投影会去努力逼近最大的那一个特征，而忽略数值比较小的特征。因为在建模前我们并不知道每个特征的重要性，这很可能导致了大量的信息缺失。为了“公平”起见，防止过分捕捉某些数值大的特征，我们会对每个特征先进行标准化处理，使得它们的大小都在相同的范围内，然后再进行PCA。|\n",
    "|Adaboost|\tAdaBoost是在每一步归一化权值的前向分步算法|\n",
    "|神经网络|\t神经网络离不开求偏导，自然也需要利用梯度下降法，所以理由与线性回归类似。|\n",
    "|GBDT|\t因为GBDT的树是在上一颗树的基础上通过梯度下降求解最优解，归一化能收敛的更快，GBDT通过减少偏差来提高性能。|\n",
    "\n",
    "\n",
    "树模型不适用归一化的原因：因为数值缩放不影响分裂点位置，对树模型的结构不造成影响，而且是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。\n",
    "\n",
    "- 如果对输出结果范围有要求，用归一化。\n",
    "\n",
    "- 如果数据较为稳定，不存在极端的最大最小值，用归一化。\n",
    "\n",
    "- 如果数据存在异常值和较多噪音，用标准化，可以间接通过中心化避免异常值和极端值的影响。\n",
    "### 具体实现\n",
    "#### 最大最小标准化（通常来说叫做广义标准化）\n",
    "- (1) 线性函数将原始数据线性化的方法转换到[0 1]的范围, 计算结果为归一化后的数据，X为原始数据\n",
    "\n",
    "- (2) 本归一化方法比较适用在数值比较集中的情况；\n",
    "\n",
    "- (3) 缺陷：如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定。实际使用中可以用经验常量来替代max和min。\n",
    "\n",
    "应用场景：在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用第一种方法或其他归一化方法（不包括Z-score方法）。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围\n",
    "#### z-score标准化（也叫广义标准化）\n",
    "$$\n",
    "    {z}^{*}=\\frac{x-μ} {σ}\n",
    "$$ \n",
    "其中，μ、σ分别为原始数据集的均值和方法。\n",
    "\n",
    "- 将原始数据集归一化为均值为0、方差1的数据集\n",
    "\n",
    "- 该种归一化方式要求原始数据的分布可以近似为高斯分布，否则归一化的效果会变得很糟糕。\n",
    "\n",
    "应用场景：在分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA技术进行降维的时候，Z-score standardization表现更好。\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e99df6600f94a5222ecb9a6a2e3edfa662ea64da68c007d01e06e6d1257b529b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
